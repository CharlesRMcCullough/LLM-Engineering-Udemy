{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHsssBgWM_l0"
   },
   "source": [
    "# Predict Product Prices\n",
    "\n",
    "## Day 3: Training!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MDyR63OTNUJ6"
   },
   "source": [
    "# pip installs\n",
    "\n",
    "# !pip install -q datasets==2.21.0 requests torch peft bitsandbytes transformers trl accelerate sentencepiece wandb matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-yikV8pRBer9",
    "ExecuteTime": {
     "end_time": "2024-12-16T15:23:02.906592Z",
     "start_time": "2024-12-16T15:22:59.286282Z"
    }
   },
   "source": [
    "# imports\n",
    "# With much thanks to Islam S. for identifying that there was a missing import!\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "#from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:14:16.843443Z",
     "start_time": "2024-12-13T23:14:16.841180Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:22:49.977929Z",
     "start_time": "2024-12-16T15:22:49.843229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:14:30.353013Z",
     "start_time": "2024-12-13T23:14:30.348792Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uuTX-xonNeOK",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:18:57.441489Z",
     "start_time": "2024-12-13T23:18:57.434784Z"
    }
   },
   "source": [
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "PROJECT_NAME = \"Pricer\"\n",
    "HF_USER = \"CharlesMac\" # your HF name here!\n",
    "\n",
    "# Data\n",
    "\n",
    "DATASET_NAME = f\"CharlesMac/lite-data\"\n",
    "# Or just use the one I've uploaded\n",
    "# DATASET_NAME = \"ed-donner/pricer-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# Run name for saving the model in the hub\n",
    "\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# Hyperparameters for QLoRA\n",
    "\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Hyperparameters for Training\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WARMUP_RATIO = 0.03\n",
    "OPTIMIZER = \"paged_adamw_32bit\"\n",
    "\n",
    "# Admin config\n",
    "\n",
    "STEPS = 50\n",
    "SAVE_STEPS = 5000\n",
    "LOG_TO_WANDB = True\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "HUB_MODEL_NAME"
   ],
   "metadata": {
    "id": "QyHOj-c4FmkM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More on Optimizers\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice\n",
    "\n",
    "The most common is Adam or AdamW (Adam with Weight Decay).  \n",
    "Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters.\n"
   ],
   "metadata": {
    "id": "PfvyitbgCMMQ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JArT3QAQAjx"
   },
   "source": [
    "### Log in to HuggingFace and Weights & Biases\n",
    "\n",
    "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
    "\n",
    "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token.\n",
    "\n",
    "Repeat this for weightsandbiases at https://wandb.ai and add a secret called `WANDB_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WyFPZeMcM88v",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:03.773070Z",
     "start_time": "2024-12-13T23:19:03.576078Z"
    }
   },
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yJNOv3cVvJ68",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:06.476122Z",
     "start_time": "2024-12-13T23:19:05.968476Z"
    }
   },
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb_api_key = os.environ['WANDB_API_KEY']\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "#\n",
    "# # Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcharles-mccullough\u001B[0m (\u001B[33mcharles-mccullough-none\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cvXVoJH8LS6u",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:18.173340Z",
     "start_time": "2024-12-13T23:19:16.558899Z"
    }
   },
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "test[0]"
   ],
   "metadata": {
    "id": "rJb9IDVjOAn9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_SUsKqA23Gc",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:23.545832Z",
     "start_time": "2024-12-13T23:19:23.135893Z"
    }
   },
   "source": [
    "if LOG_TO_WANDB:\n",
    "   wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/charles/Source/Repos/PythonProjects/UdemyCourses/LLM-Engineering-Udemy/Week7/wandb/run-20241213_171923-smo1r75l</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/charles-mccullough-none/Pricer/runs/smo1r75l' target=\"_blank\">2024-12-13_17.18.57</a></strong> to <a href='https://wandb.ai/charles-mccullough-none/Pricer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/charles-mccullough-none/Pricer' target=\"_blank\">https://wandb.ai/charles-mccullough-none/Pricer</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/charles-mccullough-none/Pricer/runs/smo1r75l' target=\"_blank\">https://wandb.ai/charles-mccullough-none/Pricer/runs/smo1r75l</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJWQ0a3wZ0Bw"
   },
   "source": [
    "## Now load the Tokenizer and Model\n",
    "\n",
    "The model is \"quantized\" - we are reducing the precision to 4 bits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9lb7M9xn46wx",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:31.121409Z",
     "start_time": "2024-12-13T23:19:31.118656Z"
    }
   },
   "source": [
    "# pick the right quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R_O04fKxMMT-",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:37.467449Z",
     "start_time": "2024-12-13T23:19:34.027899Z"
    }
   },
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c8c4e50bac042c99bfc6c70239f86cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5591.5 MB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Collator\n",
    "\n",
    "It's important that we ensure during Training that we are not trying to train the model to predict the description of products; only their price.\n",
    "\n",
    "We need to tell the trainer that everything up to \"Price is $\" is there to give context to the model to predict the next token, but does not need to be learned.\n",
    "\n",
    "The trainer needs to teach the model to predict the token(s) after \"Price is $\".\n",
    "\n",
    "There is a complicated way to do this by setting Masks, but luckily HuggingFace provides a super simple helper class to take care of this for us."
   ],
   "metadata": {
    "id": "9BYO0If4uWys"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2omVEaPIVJZa",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:19:46.599169Z",
     "start_time": "2024-12-13T23:19:46.592120Z"
    }
   },
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "response_template = \"Price is $\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AND NOW\n",
    "\n",
    "## We set up the configuration for Training\n",
    "\n",
    "We need to create 2 objects:\n",
    "\n",
    "A LoraConfig object with our hyperparameters for LoRA\n",
    "\n",
    "An SFTConfig with our overall Training parameters"
   ],
   "metadata": {
    "id": "4DaOeBhyy9eS"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fCwmDmkSATvj",
    "ExecuteTime": {
     "end_time": "2024-12-13T23:20:13.978602Z",
     "start_time": "2024-12-13T23:20:13.301376Z"
    }
   },
   "source": [
    "# First, specify the configuration parameters for LoRA\n",
    "\n",
    "lora_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Next, specify the general configuration parameters for training\n",
    "\n",
    "train_parameters = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=OPTIMIZER,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    hub_private_repo=True\n",
    ")\n",
    "\n",
    "# And now, the Supervised Fine Tuning Trainer will carry out the fine-tuning\n",
    "# Given these 2 sets of configuration parameters\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train,\n",
    "    peft_config=lora_parameters,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_parameters,\n",
    "    data_collator=collator\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# Fine-tune!\n",
    "fine_tuning.train()\n",
    "fine_tuning.model.save_pretrained(PROJECT_RUN_NAME)\n",
    "# Push our fine-tuned model to Hugging Face\n",
    "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
   ],
   "metadata": {
    "id": "GfvAxnXPvB7w",
    "ExecuteTime": {
     "end_time": "2024-12-14T01:32:59.278579Z",
     "start_time": "2024-12-13T23:20:19.057982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 2:12:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.284800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.176100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Adding directory to artifact (./Pricer-2024-12-13_17.18.57/checkpoint-5000)... Done. 0.3s\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Adding directory to artifact (./Pricer-2024-12-13_17.18.57/checkpoint-10000)... Done. 0.3s\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Adding directory to artifact (./Pricer-2024-12-13_17.18.57/checkpoint-12500)... Done. 0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d59310e0ddd84767ba17a924b6f66416"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to the hub: Pricer-2024-12-13_17.18.57\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "32vvrYRVAUNg",
    "ExecuteTime": {
     "end_time": "2024-12-14T01:37:01.738098Z",
     "start_time": "2024-12-14T01:33:22.174510Z"
    }
   },
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇████</td></tr><tr><td>train/grad_norm</td><td>▃▄▇▇▁▄▄▆▄▁▄▄▆▄▁▂▁▃▅▂▃▅▃▃▆▅▅▆▄▅▄▄▆▄▆▄█▄▅▅</td></tr><tr><td>train/learning_rate</td><td>▇███████▇▇▇▇▇▆▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▃▃▄▃▃▃▃▃▃▃▃▂▂▃▂▂▃▃▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4.013169463829791e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>12500</td></tr><tr><td>train/grad_norm</td><td>4.40602</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.1761</td></tr><tr><td>train_loss</td><td>1.23583</td></tr><tr><td>train_runtime</td><td>7735.069</td></tr><tr><td>train_samples_per_second</td><td>6.464</td></tr><tr><td>train_steps_per_second</td><td>1.616</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-12-13_17.18.57</strong> at: <a href='https://wandb.ai/charles-mccullough-none/Pricer/runs/smo1r75l' target=\"_blank\">https://wandb.ai/charles-mccullough-none/Pricer/runs/smo1r75l</a><br> View project at: <a href='https://wandb.ai/charles-mccullough-none/Pricer' target=\"_blank\">https://wandb.ai/charles-mccullough-none/Pricer</a><br>Synced 5 W&B file(s), 0 media file(s), 28 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241213_171923-smo1r75l/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorchTrain",
   "name": "pytorchtrain",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
